"""
Ollama (Llama 3.1) - LLM vs RAG+LLM ë¹„êµ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
"""

import json
import time
import os
import sys
import re
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))
from src.embedding.vectordb import VectorStore

# Ollama ì—°ë™
import requests


class OllamaClient:
    """Ollama API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, base_url: str = "http://localhost:11434", model: str = "llama3.1:8b-instruct-q8_0"):
        self.base_url = base_url
        self.model = model

    def generate(self, prompt: str, system: str = None, temperature: float = 0.1) -> str:
        """Ollama APIë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        url = f"{self.base_url}/api/generate"

        full_prompt = prompt
        if system:
            full_prompt = f"System: {system}\n\nUser: {prompt}\n\nAssistant:"

        payload = {
            "model": self.model,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "top_p": 0.9,
                "num_predict": 512,
            }
        }

        response = requests.post(url, json=payload, timeout=120)
        response.raise_for_status()

        return response.json().get("response", "")


def clean_korean_text(text: str) -> str:
    """í•œê¸€, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ë‚¨ê¸°ê³  í•„í„°ë§"""
    cleaned = re.sub(r'[^\uAC00-\uD7A3\u3131-\u3163\u1100-\u11FF0-9\s.,!?:;\'"()\-/]', '', text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()

    korean_chars = re.findall(r'[\uAC00-\uD7A3]', cleaned)
    if len(korean_chars) < 10:
        return text

    return cleaned


def calculate_keyword_score(response: str, keywords: list[str]) -> float:
    """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
    if not keywords:
        return 0.0

    response_lower = response.lower()
    matched = sum(1 for kw in keywords if kw.lower() in response_lower)
    return matched / len(keywords)


def run_evaluation(
    test_data_path: str,
    output_dir: str,
    sample_size: int = None,
    ollama_model: str = "llama3.1:8b-instruct-q8_0",
    chroma_host: str = "211.231.121.68",
    chroma_port: int = 8081,
):
    """í‰ê°€ ì‹¤í–‰"""

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    test_cases = test_data["test_cases"]
    if sample_size:
        test_cases = test_cases[:sample_size]

    print(f"\n{'='*60}")
    print(f"ğŸ¦™ Ollama LLM vs RAG+LLM ë¹„êµ í‰ê°€")
    print(f"{'='*60}")
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
    print(f"ëª¨ë¸: {ollama_model}")
    print(f"ChromaDB: {chroma_host}:{chroma_port}")
    print(f"{'='*60}\n")

    # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    print("1. Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”...")
    ollama = OllamaClient(model=ollama_model)

    # ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        test_response = ollama.generate("ì•ˆë…•", temperature=0.1)
        print(f"   âœ… Ollama ì—°ê²° ì„±ê³µ: {test_response[:50]}...")
    except Exception as e:
        print(f"   âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
        return

    # VectorStore ì´ˆê¸°í™”
    print("\n2. VectorStore ì´ˆê¸°í™” (ì›ê²© ChromaDB)...")
    try:
        vector_store = VectorStore(
            chroma_host=chroma_host,
            chroma_port=chroma_port,
            collection_name="korean_legal_docs",
            embedding_model="BAAI/bge-m3",
            device="cuda",
        )
        print(f"   âœ… VectorDB ì—°ê²° ì„±ê³µ: {vector_store.count()}ê°œ ë¬¸ì„œ")
    except Exception as e:
        print(f"   âŒ VectorDB ì—°ê²° ì‹¤íŒ¨: {e}")
        print("   ë¡œì»¬ ChromaDB ì‹œë„ ì¤‘...")
        vector_store = VectorStore(
            persist_directory="./data/embeddings/chroma",
            collection_name="korean_legal_docs",
            embedding_model="BAAI/bge-m3",
            device="cuda",
        )
        print(f"   âœ… ë¡œì»¬ VectorDB ì—°ê²° ì„±ê³µ: {vector_store.count()}ê°œ ë¬¸ì„œ")

    results = []

    print(f"\n3. í‰ê°€ ì‹œì‘ ({len(test_cases)}ê°œ ì§ˆë¬¸)...\n")

    system_prompt_llm = """ë‹¹ì‹ ì€ í•œêµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

    system_prompt_rag = """ë‹¹ì‹ ì€ í•œêµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì°¸ê³  ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ "ì°¸ê³  ìë£Œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

    for i, test_case in enumerate(test_cases):
        question = test_case["question"]
        category = test_case["category"]
        subcategory = test_case["subcategory"]
        expected_keywords = test_case.get("expected_keywords", [])

        print(f"\n{'='*60}", flush=True)
        print(f"[{i+1}/{len(test_cases)}] {category}/{subcategory}", flush=True)
        print(f"  ì§ˆë¬¸: {question[:60]}...", flush=True)
        print(f"-"*60, flush=True)

        # 1. LLM ë‹¨ë… ì‘ë‹µ
        print(f"  ğŸ“ LLM Only ì¶”ë¡  ì¤‘...", flush=True)
        start_time = time.time()
        try:
            llm_response = ollama.generate(question, system=system_prompt_llm, temperature=0.1)
            llm_response = clean_korean_text(llm_response)
        except Exception as e:
            llm_response = f"ì˜¤ë¥˜: {e}"
        llm_time = time.time() - start_time
        print(f"     âœ… LLM: {llm_time:.1f}ì´ˆ | {llm_response[:80]}...", flush=True)

        # 2. RAG ê²€ìƒ‰
        print(f"  ğŸ” RAG ê²€ìƒ‰ ì¤‘...", flush=True)
        start_time = time.time()
        try:
            retrieved_docs = vector_store.search(question, top_k=3)

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            for j, doc in enumerate(retrieved_docs, 1):
                metadata = doc.get("metadata", {})
                case_info = f"[{metadata.get('case_name', 'íŒë¡€')} - {metadata.get('court', '')}]"
                context_parts.append(f"{j}. {case_info}\n{doc['content'][:500]}")

            context = "\n\n".join(context_parts)

            # RAG í”„ë¡¬í”„íŠ¸
            rag_prompt = f"""ì°¸ê³  ìë£Œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

            rag_response = ollama.generate(rag_prompt, system=system_prompt_rag, temperature=0.1)
            rag_response = clean_korean_text(rag_response)

        except Exception as e:
            rag_response = f"ì˜¤ë¥˜: {e}"
            retrieved_docs = []
            context = ""

        rag_time = time.time() - start_time
        print(f"     âœ… RAG: {rag_time:.1f}ì´ˆ | {rag_response[:80]}...", flush=True)

        # ì ìˆ˜ ê³„ì‚°
        llm_score = calculate_keyword_score(llm_response, expected_keywords)
        rag_score = calculate_keyword_score(rag_response, expected_keywords)

        # ê²°ê³¼ ì €ì¥
        result = {
            "id": test_case["id"],
            "category": category,
            "subcategory": subcategory,
            "question": question,
            "expected_keywords": expected_keywords,
            "llm_response": llm_response,
            "rag_response": rag_response,
            "llm_time": round(llm_time, 2),
            "rag_time": round(rag_time, 2),
            "llm_keyword_score": llm_score,
            "rag_keyword_score": rag_score,
            "llm_length": len(llm_response),
            "rag_length": len(rag_response),
            "context_length": len(context) if context else 0,
            "sources_count": len(retrieved_docs),
        }
        results.append(result)

        # ê²°ê³¼ ì¶œë ¥
        winner = "RAG ğŸ‘‘" if rag_score > llm_score else "LLM ğŸ‘‘" if llm_score > rag_score else "ë™ì  ğŸ¤"
        print(f"  ğŸ“Š LLM={llm_score:.2f} vs RAG={rag_score:.2f} â†’ {winner}", flush=True)

        # ì§„í–‰ë¥ 
        completed = i + 1
        elapsed_total = sum(r['llm_time'] + r['rag_time'] for r in results)
        avg_per_q = elapsed_total / completed
        remaining = len(test_cases) - completed
        eta_min = (remaining * avg_per_q) / 60
        print(f"  â±ï¸  {completed}/{len(test_cases)} ì™„ë£Œ | ETA: {eta_min:.1f}ë¶„", flush=True)

    # ê²°ê³¼ ì €ì¥
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    result_file = os.path.join(output_dir, f"ollama_eval_{timestamp}.json")
    with open(result_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # ìš”ì•½ ê³„ì‚°
    summary = calculate_summary(results)

    summary_file = os.path.join(output_dir, f"ollama_summary_{timestamp}.json")
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    # ë¦¬í¬íŠ¸ ì¶œë ¥
    print_report(summary, results)

    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {result_file}")
    print(f"ğŸ“ ìš”ì•½ ì €ì¥: {summary_file}")

    return results, summary


def calculate_summary(results: list[dict]) -> dict:
    """ìš”ì•½ í†µê³„ ê³„ì‚°"""
    total = len(results)

    llm_scores = [r["llm_keyword_score"] for r in results]
    rag_scores = [r["rag_keyword_score"] for r in results]
    llm_times = [r["llm_time"] for r in results]
    rag_times = [r["rag_time"] for r in results]

    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    categories = {}
    for r in results:
        cat = r["category"]
        if cat not in categories:
            categories[cat] = {"llm": [], "rag": []}
        categories[cat]["llm"].append(r["llm_keyword_score"])
        categories[cat]["rag"].append(r["rag_keyword_score"])

    # ìŠ¹íŒ¨
    rag_wins = sum(1 for r in results if r["rag_keyword_score"] > r["llm_keyword_score"])
    llm_wins = sum(1 for r in results if r["llm_keyword_score"] > r["rag_keyword_score"])
    ties = total - rag_wins - llm_wins

    return {
        "total_tests": total,
        "model": "llama3.1:8b-instruct-q8_0",
        "overall": {
            "llm_keyword_avg": sum(llm_scores) / total,
            "rag_keyword_avg": sum(rag_scores) / total,
            "llm_time_avg": sum(llm_times) / total,
            "rag_time_avg": sum(rag_times) / total,
            "improvement": (sum(rag_scores) - sum(llm_scores)) / total,
        },
        "comparison": {
            "rag_wins": rag_wins,
            "llm_wins": llm_wins,
            "ties": ties,
            "rag_win_rate": rag_wins / total * 100,
        },
        "by_category": {
            cat: {
                "llm_avg": sum(data["llm"]) / len(data["llm"]),
                "rag_avg": sum(data["rag"]) / len(data["rag"]),
                "improvement": (sum(data["rag"]) - sum(data["llm"])) / len(data["llm"]),
            }
            for cat, data in categories.items()
        },
    }


def print_report(summary: dict, results: list[dict]):
    """í‰ê°€ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ¦™ Ollama LLM vs RAG+LLM ë¹„êµ í‰ê°€ ê²°ê³¼")
    print("="*60)

    print(f"\nâ–¶ ëª¨ë¸: {summary['model']}")
    print(f"â–¶ ì´ í…ŒìŠ¤íŠ¸: {summary['total_tests']}ê°œ")

    print(f"\nâ–¶ ì „ì²´ ì ìˆ˜ (í‚¤ì›Œë“œ ë§¤ì¹­)")
    print(f"  LLM Only:  {summary['overall']['llm_keyword_avg']:.3f}")
    print(f"  RAG + LLM: {summary['overall']['rag_keyword_avg']:.3f}")
    improvement_pct = summary['overall']['improvement'] * 100
    arrow = "â†‘" if improvement_pct > 0 else "â†“"
    print(f"  ê°œì„ ìœ¨: {arrow}{abs(improvement_pct):.1f}%")

    print(f"\nâ–¶ ì‘ë‹µ ì‹œê°„")
    print(f"  LLM Only:  {summary['overall']['llm_time_avg']:.2f}ì´ˆ")
    print(f"  RAG + LLM: {summary['overall']['rag_time_avg']:.2f}ì´ˆ")

    print(f"\nâ–¶ ìŠ¹íŒ¨ (í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ì¤€)")
    print(f"  RAG ìš°ì„¸: {summary['comparison']['rag_wins']}ê±´ ({summary['comparison']['rag_win_rate']:.1f}%)")
    print(f"  LLM ìš°ì„¸: {summary['comparison']['llm_wins']}ê±´")
    print(f"  ë™ì : {summary['comparison']['ties']}ê±´")

    print(f"\nâ–¶ ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼")
    for cat, data in summary["by_category"].items():
        imp = data["improvement"] * 100
        arrow = "â†‘" if imp > 0 else "â†“" if imp < 0 else "="
        print(f"  {cat}: LLM={data['llm_avg']:.3f} â†’ RAG={data['rag_avg']:.3f} ({arrow}{abs(imp):.1f}%)")

    print("\n" + "="*60)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Ollama LLM vs RAG+LLM ë¹„êµ í‰ê°€")
    parser.add_argument("--test-data", default="./data/test_dataset_case_specific_200.json")
    parser.add_argument("--output-dir", default="./data/evaluation_ollama")
    parser.add_argument("--sample-size", type=int, default=None)
    parser.add_argument("--model", default="llama3.1:8b-instruct-q8_0")
    parser.add_argument("--chroma-host", default="211.231.121.68")
    parser.add_argument("--chroma-port", type=int, default=8081)

    args = parser.parse_args()

    run_evaluation(
        test_data_path=args.test_data,
        output_dir=args.output_dir,
        sample_size=args.sample_size,
        ollama_model=args.model,
        chroma_host=args.chroma_host,
        chroma_port=args.chroma_port,
    )
