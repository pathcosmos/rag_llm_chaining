"""
LLM vs RAG+LLM ë¹„êµ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
RAG ì¶”ê°€ì˜ íš¨ê³¼ë¥¼ ì •ëŸ‰ì /ì •ì„±ì ìœ¼ë¡œ ì¸¡ì •
"""

import json
import time
import os
import sys
import re
from datetime import datetime
from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))
from src.embedding.vectordb import VectorStore


def clean_korean_text(text: str) -> str:
    """í•œê¸€, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ë‚¨ê¸°ê³  í•„í„°ë§

    - í•œê¸€ (ê°€-í£)
    - ìˆ«ì (0-9)
    - ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ (., ,, !, ?, :, ;, ', ", (, ), -, /)
    - ê³µë°±
    - í•œê¸€ì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì›ë³¸ ë°˜í™˜
    """
    # í•œê¸€, ìˆ«ì, ê³µë°±, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ìœ ì§€
    cleaned = re.sub(r'[^\uAC00-\uD7A3\u3131-\u3163\u1100-\u11FF0-9\s.,!?:;\'"()\-/]', '', text)
    # ì—°ì† ê³µë°± ì œê±°
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()

    # í•œê¸€ ê¸€ì ìˆ˜ í™•ì¸ (ìµœì†Œ 10ê¸€ì ì´ìƒì´ì–´ì•¼ ìœ íš¨)
    korean_chars = re.findall(r'[\uAC00-\uD7A3]', cleaned)
    if len(korean_chars) < 10:
        # í•œê¸€ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ ë°˜í™˜ (ì˜ì–´ ì‘ë‹µì¸ ê²½ìš°)
        return text

    return cleaned


class LLMOnlyGenerator:
    """LLMë§Œ ì‚¬ìš©í•˜ëŠ” ìƒì„±ê¸° (RAG ì—†ìŒ)"""

    def __init__(
        self,
        model_name: str = "Qwen/Qwen2.5-7B-Instruct",
        device: str = "cuda",
    ):
        print(f"LLM ë¡œë”©: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
        print("LLM ë¡œë”© ì™„ë£Œ!")

    def generate(
        self,
        query: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
    ) -> str:
        """LLM ë‹¨ë… ì‘ë‹µ ìƒì„± (RAG ì—†ìŒ)"""
        system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì˜ì–´, í•œì, ì¼ë³¸ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query},
        ]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True,
        )
        # í•œê¸€ í•„í„°ë§ ì ìš©
        return clean_korean_text(response)


class RAGGenerator:
    """RAG+LLM ìƒì„±ê¸°"""

    def __init__(
        self,
        model_name: str = "Qwen/Qwen2.5-7B-Instruct",
        embedding_model: str = "BAAI/bge-m3",
        chroma_host: str = "211.231.121.68",
        chroma_port: int = 8081,
        collection_name: str = "korean_legal_docs",
        device: str = "cuda",
        use_4bit: bool = True,
    ):
        print(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")

        # VectorStore ì´ˆê¸°í™” (ì›ê²© ChromaDB)
        self.vector_store = VectorStore(
            chroma_host=chroma_host,
            chroma_port=chroma_port,
            collection_name=collection_name,
            embedding_model=embedding_model,
            device=device,
        )

        # LLM ë¡œë”© (4bit ì–‘ìí™”ë¡œ ì†ë„ í–¥ìƒ)
        print(f"LLM ë¡œë”©: {model_name} (4bit={use_4bit})")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
        )

        if use_4bit:
            from transformers import BitsAndBytesConfig
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True,
            )
        print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")

    def retrieve(self, query: str, top_k: int = 5) -> list[dict]:
        """ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        return self.vector_store.search(query, top_k=top_k)

    def generate(
        self,
        query: str,
        top_k: int = 3,
        max_new_tokens: int = 256,
        temperature: float = 0.1,
    ) -> dict:
        """RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        # 1. ê²€ìƒ‰
        retrieved_docs = self.retrieve(query, top_k=top_k)

        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for i, doc in enumerate(retrieved_docs, 1):
            metadata = doc.get("metadata", {})
            case_info = f"[{metadata.get('case_name', 'íŒë¡€')} - {metadata.get('court', '')}]"
            context_parts.append(f"{i}. {case_info}\n{doc['content'][:500]}")

        context = "\n\n".join(context_parts)

        # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì°¸ê³  ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ "ì°¸ê³  ìë£Œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì˜ì–´, í•œì, ì¼ë³¸ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."""

        user_prompt = f"""ì°¸ê³  ìë£Œ:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True,
        )

        # í•œê¸€ í•„í„°ë§ ì ìš©
        return {
            "answer": clean_korean_text(response),
            "sources": retrieved_docs,
            "context_length": len(context),
        }


def calculate_keyword_score(response: str, keywords: list[str]) -> float:
    """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
    if not keywords:
        return 0.0

    response_lower = response.lower()
    matched = sum(1 for kw in keywords if kw.lower() in response_lower)
    return matched / len(keywords)


def evaluate_response_quality(
    question: str,
    llm_response: str,
    rag_response: str,
    expected_keywords: list[str],
) -> dict:
    """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
    return {
        "llm_length": len(llm_response),
        "rag_length": len(rag_response),
        "llm_keyword_score": calculate_keyword_score(llm_response, expected_keywords),
        "rag_keyword_score": calculate_keyword_score(rag_response, expected_keywords),
    }


def run_evaluation(
    test_data_path: str,
    output_dir: str,
    sample_size: int = None,
    model_name: str = "Qwen/Qwen2.5-7B-Instruct",
):
    """í‰ê°€ ì‹¤í–‰"""
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    test_cases = test_data["test_cases"]
    if sample_size:
        test_cases = test_cases[:sample_size]

    print(f"\n{'='*60}")
    print(f"LLM vs RAG+LLM ë¹„êµ í‰ê°€")
    print(f"{'='*60}")
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
    print(f"ëª¨ë¸: {model_name}")
    print(f"{'='*60}\n")

    # ìƒì„±ê¸° ì´ˆê¸°í™” (ëª¨ë¸ ê³µìœ ë¥¼ ìœ„í•´ ìˆœì°¨ ì´ˆê¸°í™”)
    print("1. RAG ìƒì„±ê¸° ì´ˆê¸°í™”...")
    rag_gen = RAGGenerator(model_name=model_name)

    # LLM ìƒì„±ê¸°ëŠ” RAG ìƒì„±ê¸°ì˜ ëª¨ë¸ ì¬ì‚¬ìš©
    print("\n2. LLM ìƒì„±ê¸° ì„¤ì • (ëª¨ë¸ ê³µìœ )...")

    results = []

    print(f"\n3. í‰ê°€ ì‹œì‘ ({len(test_cases)}ê°œ ì§ˆë¬¸)...\n")

    for i, test_case in enumerate(test_cases):
        question = test_case["question"]
        category = test_case["category"]
        subcategory = test_case["subcategory"]
        expected_keywords = test_case.get("expected_keywords", [])

        print(f"\n{'='*60}", flush=True)
        print(f"[{i+1}/{len(test_cases)}] {category}/{subcategory}", flush=True)
        print(f"  ì§ˆë¬¸: {question}", flush=True)
        print(f"  ì˜ˆìƒ í‚¤ì›Œë“œ: {expected_keywords}", flush=True)
        print(f"-"*60, flush=True)

        # LLM ë‹¨ë… ì‘ë‹µ
        print(f"  ğŸ“ LLM ì¶”ë¡  ì‹œì‘...", flush=True)
        start_time = time.time()
        llm_system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì˜ì–´, í•œì, ì¼ë³¸ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."""

        llm_messages = [
            {"role": "system", "content": llm_system_prompt},
            {"role": "user", "content": question},
        ]
        llm_text = rag_gen.tokenizer.apply_chat_template(
            llm_messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        llm_inputs = rag_gen.tokenizer(llm_text, return_tensors="pt").to(rag_gen.model.device)

        with torch.no_grad():
            llm_outputs = rag_gen.model.generate(
                **llm_inputs,
                max_new_tokens=256,
                temperature=0.1,
                top_p=0.9,
                do_sample=True,
                pad_token_id=rag_gen.tokenizer.pad_token_id,
            )

        llm_response = rag_gen.tokenizer.decode(
            llm_outputs[0][llm_inputs.input_ids.shape[1]:],
            skip_special_tokens=True,
        )
        # í•œê¸€ í•„í„°ë§ ì ìš©
        llm_response = clean_korean_text(llm_response)
        llm_time = time.time() - start_time
        print(f"  âœ… LLM ì™„ë£Œ: {llm_time:.1f}ì´ˆ", flush=True)
        print(f"  ğŸ“„ LLM ì‘ë‹µ (ì• 200ì): {llm_response[:200]}...", flush=True)

        # RAG+LLM ì‘ë‹µ
        print(f"  ğŸ” RAG ê²€ìƒ‰ + LLM ì¶”ë¡  ì‹œì‘...", flush=True)
        start_time = time.time()
        rag_result = rag_gen.generate(question, top_k=3)
        rag_response = rag_result["answer"]
        rag_time = time.time() - start_time
        print(f"  âœ… RAG ì™„ë£Œ: {rag_time:.1f}ì´ˆ", flush=True)
        print(f"  ğŸ“„ RAG ì‘ë‹µ (ì• 200ì): {rag_response[:200]}...", flush=True)

        # í’ˆì§ˆ í‰ê°€
        quality = evaluate_response_quality(
            question, llm_response, rag_response, expected_keywords
        )

        result = {
            "id": test_case["id"],
            "category": category,
            "subcategory": subcategory,
            "question": question,
            "expected_keywords": expected_keywords,
            "llm_response": llm_response,
            "rag_response": rag_response,
            "llm_time": round(llm_time, 2),
            "rag_time": round(rag_time, 2),
            "llm_keyword_score": quality["llm_keyword_score"],
            "rag_keyword_score": quality["rag_keyword_score"],
            "llm_length": quality["llm_length"],
            "rag_length": quality["rag_length"],
            "context_length": rag_result.get("context_length", 0),
            "sources_count": len(rag_result.get("sources", [])),
        }
        results.append(result)

        # ê²°ê³¼ ë¹„êµ ì¶œë ¥
        print(f"-"*60, flush=True)
        print(f"  ğŸ“Š ê²°ê³¼ ë¹„êµ:", flush=True)
        print(f"     LLM: ì‹œê°„={llm_time:.1f}s, í‚¤ì›Œë“œì ìˆ˜={quality['llm_keyword_score']:.2f}, ê¸¸ì´={quality['llm_length']}ì", flush=True)
        print(f"     RAG: ì‹œê°„={rag_time:.1f}s, í‚¤ì›Œë“œì ìˆ˜={quality['rag_keyword_score']:.2f}, ê¸¸ì´={quality['rag_length']}ì", flush=True)
        winner = "RAG ğŸ‘‘" if quality['rag_keyword_score'] > quality['llm_keyword_score'] else "LLM ğŸ‘‘" if quality['llm_keyword_score'] > quality['rag_keyword_score'] else "ë™ì  ğŸ¤"
        print(f"     ìŠ¹ì: {winner}", flush=True)

        # ëˆ„ì  í†µê³„
        completed = i + 1
        elapsed_total = sum(r['llm_time'] + r['rag_time'] for r in results)
        avg_per_question = elapsed_total / completed
        remaining = len(test_cases) - completed
        eta_seconds = remaining * avg_per_question
        eta_minutes = eta_seconds / 60
        print(f"  â±ï¸  ì§„í–‰: {completed}/{len(test_cases)} ì™„ë£Œ | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {eta_minutes:.1f}ë¶„", flush=True)

    # ê²°ê³¼ ì €ì¥
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ìƒì„¸ ê²°ê³¼ JSON
    result_file = os.path.join(output_dir, f"evaluation_results_{timestamp}.json")
    with open(result_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # ìš”ì•½ í†µê³„ ê³„ì‚°
    summary = calculate_summary(results)

    # ìš”ì•½ ì €ì¥
    summary_file = os.path.join(output_dir, f"evaluation_summary_{timestamp}.json")
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    # ë¦¬í¬íŠ¸ ì¶œë ¥
    print_report(summary, results)

    return results, summary


def calculate_summary(results: list[dict]) -> dict:
    """ìš”ì•½ í†µê³„ ê³„ì‚°"""
    total = len(results)

    # ì „ì²´ í†µê³„
    llm_keyword_scores = [r["llm_keyword_score"] for r in results]
    rag_keyword_scores = [r["rag_keyword_score"] for r in results]
    llm_times = [r["llm_time"] for r in results]
    rag_times = [r["rag_time"] for r in results]

    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    categories = {}
    for r in results:
        cat = r["category"]
        if cat not in categories:
            categories[cat] = {
                "count": 0,
                "llm_keyword_scores": [],
                "rag_keyword_scores": [],
            }
        categories[cat]["count"] += 1
        categories[cat]["llm_keyword_scores"].append(r["llm_keyword_score"])
        categories[cat]["rag_keyword_scores"].append(r["rag_keyword_score"])

    # ì¹´í…Œê³ ë¦¬ë³„ í‰ê· 
    for cat, data in categories.items():
        data["llm_avg"] = sum(data["llm_keyword_scores"]) / len(data["llm_keyword_scores"])
        data["rag_avg"] = sum(data["rag_keyword_scores"]) / len(data["rag_keyword_scores"])
        data["improvement"] = data["rag_avg"] - data["llm_avg"]

    # RAGê°€ ë” ì¢‹ì€ ê²½ìš°
    rag_wins = sum(1 for r in results if r["rag_keyword_score"] > r["llm_keyword_score"])
    llm_wins = sum(1 for r in results if r["llm_keyword_score"] > r["rag_keyword_score"])
    ties = total - rag_wins - llm_wins

    return {
        "total_tests": total,
        "overall": {
            "llm_keyword_avg": sum(llm_keyword_scores) / total,
            "rag_keyword_avg": sum(rag_keyword_scores) / total,
            "llm_time_avg": sum(llm_times) / total,
            "rag_time_avg": sum(rag_times) / total,
        },
        "comparison": {
            "rag_wins": rag_wins,
            "llm_wins": llm_wins,
            "ties": ties,
            "rag_win_rate": rag_wins / total * 100,
        },
        "by_category": {
            cat: {
                "count": data["count"],
                "llm_avg": round(data["llm_avg"], 3),
                "rag_avg": round(data["rag_avg"], 3),
                "improvement": round(data["improvement"], 3),
            }
            for cat, data in categories.items()
        },
    }


def print_report(summary: dict, results: list[dict]):
    """í‰ê°€ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“Š LLM vs RAG+LLM ë¹„êµ í‰ê°€ ê²°ê³¼")
    print("="*60)

    print("\nâ–¶ ì „ì²´ ìš”ì•½")
    print(f"  ì´ í…ŒìŠ¤íŠ¸: {summary['total_tests']}ê°œ")
    print(f"  LLM í‚¤ì›Œë“œ ì ìˆ˜ í‰ê· : {summary['overall']['llm_keyword_avg']:.3f}")
    print(f"  RAG í‚¤ì›Œë“œ ì ìˆ˜ í‰ê· : {summary['overall']['rag_keyword_avg']:.3f}")
    print(f"  LLM í‰ê·  ì‘ë‹µì‹œê°„: {summary['overall']['llm_time_avg']:.2f}ì´ˆ")
    print(f"  RAG í‰ê·  ì‘ë‹µì‹œê°„: {summary['overall']['rag_time_avg']:.2f}ì´ˆ")

    print("\nâ–¶ ìŠ¹íŒ¨ ë¹„êµ (í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ì¤€)")
    print(f"  RAG ìš°ì„¸: {summary['comparison']['rag_wins']}ê±´ ({summary['comparison']['rag_win_rate']:.1f}%)")
    print(f"  LLM ìš°ì„¸: {summary['comparison']['llm_wins']}ê±´")
    print(f"  ë™ì : {summary['comparison']['ties']}ê±´")

    print("\nâ–¶ ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼")
    for cat, data in summary["by_category"].items():
        improvement_pct = data["improvement"] * 100
        arrow = "â†‘" if data["improvement"] > 0 else "â†“" if data["improvement"] < 0 else "="
        print(f"  {cat}: LLM={data['llm_avg']:.3f} â†’ RAG={data['rag_avg']:.3f} ({arrow}{abs(improvement_pct):.1f}%)")

    # RAG íš¨ê³¼ê°€ í° ì˜ˆì‹œ ì¶œë ¥
    print("\nâ–¶ RAG íš¨ê³¼ê°€ í° ì˜ˆì‹œ (Top 3)")
    sorted_by_improvement = sorted(
        results,
        key=lambda x: x["rag_keyword_score"] - x["llm_keyword_score"],
        reverse=True
    )[:3]

    for i, r in enumerate(sorted_by_improvement, 1):
        improvement = r["rag_keyword_score"] - r["llm_keyword_score"]
        print(f"\n  {i}. [{r['subcategory']}]")
        print(f"     ì§ˆë¬¸: {r['question'][:40]}...")
        print(f"     LLM: {r['llm_keyword_score']:.2f} â†’ RAG: {r['rag_keyword_score']:.2f} (+{improvement:.2f})")

    print("\n" + "="*60)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="LLM vs RAG+LLM ë¹„êµ í‰ê°€")
    parser.add_argument(
        "--test-data",
        default="./data/test_dataset.json",
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ",
    )
    parser.add_argument(
        "--output-dir",
        default="./data/evaluation_results",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬",
    )
    parser.add_argument(
        "--sample-size",
        type=int,
        default=None,
        help="í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: ì „ì²´)",
    )
    parser.add_argument(
        "--model",
        default="Qwen/Qwen2.5-7B-Instruct",
        help="ì‚¬ìš©í•  LLM ëª¨ë¸",
    )

    args = parser.parse_args()

    run_evaluation(
        test_data_path=args.test_data,
        output_dir=args.output_dir,
        sample_size=args.sample_size,
        model_name=args.model,
    )
