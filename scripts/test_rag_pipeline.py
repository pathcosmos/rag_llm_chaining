#!/usr/bin/env python3
"""
RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸
VectorDB + LLM í†µí•© í…ŒìŠ¤íŠ¸
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv(project_root / ".env")


def test_rag_pipeline():
    """RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸"""
    import torch
    from src.rag.pipeline import RAGPipeline

    print("=" * 70)
    print("RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    # GPU ìƒíƒœ
    print(f"\nGPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"í˜„ì¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    print("\n" + "-" * 70)
    print("RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    print("-" * 70)

    pipeline = RAGPipeline(
        llm_model="Qwen/Qwen2.5-7B-Instruct",
        embedding_model="BAAI/bge-m3",
        vector_db_path="./data/embeddings/chroma",
        collection_name="korean_legal_docs",
        device="cuda",
    )

    print(f"\nVRAM ì‚¬ìš©ëŸ‰ (ì´ˆê¸°í™” í›„): {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ê·€ì†ì¬ì‚°ì˜ ì„ëŒ€ì°¨ ê³„ì•½ì—ì„œ ì„ì°¨ì¸ì˜ ê¶Œë¦¬ëŠ” ì–´ë–»ê²Œ ë³´í˜¸ë˜ë‚˜ìš”?",
        "ë²•ì¸ì´ í•´ì‚°ëœ ê²½ìš° ì²­ì‚° ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "í† ì§€ ì†Œìœ ê¶Œ ì´ì „ ë“±ê¸° ê´€ë ¨ íŒë¡€ê°€ ìˆë‚˜ìš”?",
    ]

    print("\n" + "=" * 70)
    print("RAG ì§ˆì˜ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    for i, question in enumerate(test_questions, 1):
        print(f"\n{'â”€' * 70}")
        print(f"[ì§ˆë¬¸ {i}] {question}")
        print("â”€" * 70)

        # RAG ì§ˆì˜ ì‹¤í–‰
        result = pipeline.query(
            question=question,
            top_k=3,
            max_new_tokens=512,
            return_sources=True,
        )

        # ë‹µë³€ ì¶œë ¥
        print(f"\nğŸ“ ë‹µë³€:")
        print(result["answer"])

        # ì†ŒìŠ¤ ì¶œë ¥
        print(f"\nğŸ“š ì°¸ê³  ìë£Œ ({len(result['sources'])}ê±´):")
        for j, source in enumerate(result["sources"], 1):
            meta = source["metadata"]
            print(f"  [{j}] {meta.get('case_name', 'N/A')} ({meta.get('court', 'N/A')})")
            print(f"      ì ìˆ˜: {source['score']:.4f}")

    # RAG vs Non-RAG ë¹„êµ
    print("\n" + "=" * 70)
    print("RAG vs Non-RAG ë¹„êµ")
    print("=" * 70)

    comparison_question = "ê·€ì†ì¬ì‚° ì„ëŒ€ì°¨ ê³„ì•½ì—ì„œ ì„ì°¨ê¶Œ í¬ê¸°ì˜ íš¨ë ¥ì€ ì–¸ì œ ë°œìƒí•˜ë‚˜ìš”?"

    print(f"\nì§ˆë¬¸: {comparison_question}")

    # RAG ì‘ë‹µ
    print("\n[RAG ì‘ë‹µ]")
    rag_result = pipeline.query(
        question=comparison_question,
        top_k=3,
        max_new_tokens=300,
    )
    print(rag_result["answer"])

    # Non-RAG ì‘ë‹µ (ì»¨í…ìŠ¤íŠ¸ ì—†ì´)
    print("\n[Non-RAG ì‘ë‹µ (ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)]")
    non_rag_answer = pipeline.generate(
        query=comparison_question,
        context="(ì°¸ê³  ìë£Œ ì—†ìŒ)",
        max_new_tokens=300,
    )
    print(non_rag_answer)

    print("\n" + "=" * 70)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del pipeline
    torch.cuda.empty_cache()


if __name__ == "__main__":
    test_rag_pipeline()
