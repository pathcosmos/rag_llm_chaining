# Ollama LLM vs RAG+LLM 비교 평가 리포트

**평가 일시**: 2024-12-08
**모델**: Llama 3.1 8B Instruct (Q8_0 양자화)
**평가 데이터**: 한국 법률 QA 200건
**VectorDB**: ChromaDB (599,652개 법률 문서)

---

## 1. 평가 개요

### 1.1 목적
로컬 LLM(Ollama)을 활용한 RAG(Retrieval-Augmented Generation) 시스템의 효과 검증

### 1.2 평가 방식
- **LLM Only**: 질문만으로 응답 생성
- **RAG + LLM**: VectorDB에서 관련 문서 검색 후 컨텍스트와 함께 응답 생성
- **평가 지표**: 키워드 매칭 점수 (예상 키워드 포함 비율)

### 1.3 시스템 구성

| 구성 요소 | 상세 |
|-----------|------|
| LLM | Ollama + Llama 3.1 8B Instruct (Q8_0) |
| Embedding | BAAI/bge-m3 (1024차원) |
| VectorDB | ChromaDB (원격: 211.231.121.68:8081) |
| 검색 Top-K | 3개 문서 |
| Temperature | 0.1 |

---

## 2. 전체 결과 요약

### 2.1 핵심 지표

| 지표 | LLM Only | RAG + LLM | 변화 |
|------|----------|-----------|------|
| **평균 키워드 점수** | 0.569 | 0.673 | **+10.4%** |
| **평균 응답 시간** | 2.94초 | 2.61초 | **-11.2%** |
| **완벽 점수 (1.0)** | 32건 | 65건 | **+103%** |
| **평균 응답 길이** | 406자 | 322자 | -20.7% |

### 2.2 승패 현황

```
┌─────────────────────────────────────────────────────────┐
│                    200건 테스트 결과                     │
├─────────────────────────────────────────────────────────┤
│  RAG 우세    ████████████████████████████████░░  78건 (39.0%)
│  LLM 우세    ████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  19건 (9.5%)
│  동점        ████████████████████████████████████████░░░░  103건 (51.5%)
└─────────────────────────────────────────────────────────┘
```

**RAG vs LLM 승률**: **4.1 : 1** (RAG가 4배 이상 우세)

---

## 3. 카테고리별 분석

### 3.1 카테고리별 성능 비교

| 카테고리 | LLM 점수 | RAG 점수 | 개선율 | 건수 |
|----------|----------|----------|--------|------|
| case_specific (판례) | 0.569 | 0.659 | +9.0% | 76건 |
| legal_principle (법리) | 0.547 | 0.659 | +11.2% | 76건 |
| procedure_detail (절차) | 0.603 | 0.718 | +11.6% | 48건 |

### 3.2 카테고리별 인사이트

#### 판례 (case_specific)
- 특정 판례의 판시사항, 당사자 관계 등 **사실 관계** 질문
- RAG가 관련 판례 원문을 직접 참조하여 정확도 향상

#### 법리 (legal_principle)
- 법률 원칙, 조문 해석, 법리 적용 질문
- RAG가 대법원 판례의 **법리 해석** 부분을 검색하여 활용

#### 절차 (procedure_detail)
- 등기, 소송, 행정 절차 관련 세부 질문
- **가장 높은 개선율 (+11.6%)**: 절차적 세부사항은 학습 데이터보다 검색 문서가 더 정확

---

## 4. 상세 분석

### 4.1 점수 분포

| 점수 범위 | LLM Only | RAG + LLM |
|-----------|----------|-----------|
| 0.0 (0%) | 14건 | 11건 |
| 0.1-0.4 | 42건 | 26건 |
| 0.5-0.7 | 83건 | 69건 |
| 0.8-0.9 | 29건 | 29건 |
| 1.0 (100%) | 32건 | **65건** |

### 4.2 극단 케이스 분석

#### 최대 개선 케이스 (+1.0)
- **질문**: "용역의 공급시기 결정 요인 중 추가로 제공되는 역무 조건..."
- **LLM**: 0.00 → **RAG**: 1.00
- **분석**: LLM은 일반적인 답변만 제공, RAG는 관련 판례에서 정확한 조건 추출

#### 최대 악화 케이스 (-0.67)
- **질문**: "구 관세법 제234조 제1호의 '풍속을 해치는' 의미..."
- **LLM**: 0.67 → **RAG**: 0.00
- **분석**: 검색된 문서가 질문과 무관한 판례여서 오히려 혼란 야기

### 4.3 응답 특성

| 특성 | LLM Only | RAG + LLM |
|------|----------|-----------|
| 평균 응답 길이 | 406자 | 322자 |
| 응답 스타일 | 일반적 설명 | 문서 기반 답변 |
| 환각 발생률 | 높음 | 낮음 |
| 출처 명시 | 없음 | 판례명 포함 |

---

## 5. 성능 분석

### 5.1 응답 시간 비교

| 단계 | LLM Only | RAG + LLM |
|------|----------|-----------|
| 임베딩 생성 | - | ~0.3초 |
| VectorDB 검색 | - | ~0.5초 |
| LLM 추론 | 2.94초 | 1.81초 |
| **총 시간** | 2.94초 | **2.61초** |

**RAG가 더 빠른 이유**: 검색된 컨텍스트가 제공되어 LLM이 추론할 범위가 줄어듦

### 5.2 리소스 사용량

| 리소스 | 수치 |
|--------|------|
| GPU | RTX 5070 Ti (16GB VRAM) |
| LLM 메모리 | ~8GB (Q8_0) |
| Embedding 메모리 | ~2GB (bge-m3) |
| VectorDB 문서 수 | 599,652개 |

---

## 6. 결론 및 권장사항

### 6.1 핵심 결론

1. **RAG 효과 입증**: 10.4% 키워드 매칭 개선
2. **완벽 응답 2배 증가**: 32건 → 65건
3. **속도 개선**: RAG 추가에도 11.2% 빠른 응답
4. **4:1 승률**: RAG가 LLM 단독보다 4배 우세

### 6.2 RAG가 특히 효과적인 경우

- 특정 판례/법조문 인용이 필요한 질문
- 절차적 세부사항 (기한, 요건, 순서)
- 최신 법률 정보가 필요한 경우
- 정확한 숫자/날짜 정보 요구

### 6.3 개선 필요 사항

1. **검색 정밀도 향상**: 일부 케이스에서 무관한 문서 검색
2. **Reranker 도입**: 검색 결과 순위 재조정
3. **HyDE 적용**: 가설 문서 생성으로 검색 품질 개선
4. **컨텍스트 길이 최적화**: 현재 Top-3 → 동적 조절

### 6.4 다음 단계

- [ ] Reranker 모델 통합 (bge-reranker-v2-m3)
- [ ] 하이브리드 검색 (Dense + Sparse)
- [ ] 평가 지표 확장 (RAGAS, 인간 평가)
- [ ] 프로덕션 API 서버 구축

---

## 7. 부록

### 7.1 테스트 환경

```yaml
Hardware:
  GPU: NVIDIA GeForce RTX 5070 Ti (16GB)
  CPU: 20 cores
  RAM: 125.5 GB

Software:
  OS: Ubuntu 24.04.3 LTS
  Python: 3.12
  Ollama: 0.13.0
  ChromaDB: Remote (Docker)

Models:
  LLM: llama3.1:8b-instruct-q8_0
  Embedding: BAAI/bge-m3
```

### 7.2 데이터 파일

| 파일 | 경로 |
|------|------|
| 평가 결과 (전체) | `data/evaluation_ollama/ollama_eval_20251208_225047.json` |
| 요약 통계 | `data/evaluation_ollama/ollama_summary_20251208_225047.json` |
| 테스트 데이터셋 | `data/test_dataset_case_specific_200.json` |

### 7.3 관련 스크립트

| 스크립트 | 용도 |
|----------|------|
| `scripts/evaluate_ollama_rag.py` | Ollama 평가 스크립트 |
| `src/embedding/vectordb.py` | VectorStore 클래스 |

---

*이 리포트는 RAG + LLM Chaining 프로젝트의 일환으로 생성되었습니다.*
